{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "앞서 배운 메모리 클래스 중 하나를 사용하는 메모리로 LCEL 체인을 구현합니다.\n",
    "이 체인은 영화 제목을 가져와 영화를 나타내는 세 개의 이모티콘으로 응답해야 합니다. (예: \"탑건\" -> \"🛩️👨‍✈️🔥\". \"대부\" -> \"👨‍👨‍👦🔫🍝\").\n",
    "항상 세 개의 이모티콘으로 답장하도록 FewShotPromptTemplate 또는 FewShotChatMessagePromptTemplate을 사용하여 체인에 예시를 제공하세요.\n",
    "메모리가 작동하는지 확인하려면 체인에 두 개의 영화에 대해 질문한 다음 다른 셀에서 체인에 먼저 질문한 영화가 무엇인지 알려달라고 요청하세요.\n",
    "\"\"\"\n",
    "\n",
    "# Chat\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "# Prompt\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "temperature=0.1,\n",
    ")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral:latest\",\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"How can you change the title of 'Topgun' to emoji?\",\n",
    "\t\t\"answer\": \"🛩️👨‍✈️🔥\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"question\": \"How can you change the title of 'Godfather' to emoji?\",\n",
    "\t\t\"answer\": \"👨‍👨‍👦🔫🍝\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"question\": \"How can you change the title of 'HarryPoter' to emoji?\",\n",
    "\t\t\"answer\": \"🔮🏰🧙\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a movieholic and know every scenario of the movie. \n",
    "    You have to change the titile of movie to emogi, considering the movie's concept. \n",
    "    You must only use 3 emojis to represent the movie without any texts. Not explanation, Not note, You must use only 3 emojis in this responce.\n",
    "    \n",
    "    \n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "\texample_prompt=example_prompt,\n",
    "\texamples=examples,\n",
    "\tsuffix=\"How can you change the title of {movie} to emoji?\",\n",
    "\tinput_variables=[\"movie\"]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "\tllm = llm,\n",
    "\tmax_token_limit = 100,\n",
    "\tmemory_key='chat_history',\n",
    "\treturn_messages = True,\n",
    "\t)\n",
    "\n",
    "def load_memory():\n",
    "\treturn memory.load_memory_variables({})[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_chain(title):\n",
    "    m_title = title\n",
    "    result = chain.invoke({\"movie\": m_title})\n",
    "    print(result.content)\n",
    "    \n",
    "    memory.save_context({\"input\":f\"{m_title}\"}, {\"output\":result.content},)\n",
    "\n",
    "def get_result(title):\n",
    "\n",
    "    if load_memory() == [] :\n",
    "        result = invoke_chain(title)\n",
    "        \n",
    "        print(result)\n",
    "        print('\\n')\n",
    "        print('[Recent Search]')\n",
    "        print('No history')\n",
    "\n",
    "    else:\n",
    "        result = invoke_chain(title)\n",
    "\n",
    "        print(result)\n",
    "        print('\\n')\n",
    "        print('[Recent Search]')\n",
    "        print(str(load_memory()[-4]).replace(\"content='\",\"\").replace(\"'\",\"\"), \":\", str(load_memory()[-3]).replace(\"content='\",\"\").replace(\"'\",\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 🌊� kennis 🦈 (or 🐬 for dolphin if you prefer)\n",
      "None\n",
      "\n",
      "\n",
      "[Recent Search]\n",
      "No history\n"
     ]
    }
   ],
   "source": [
    "get_result('Aquaman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 👨‍🚀🔵💥\n",
      "\n",
      "(Note: The blue S represents the \"S\" in Superman.)\n",
      "None\n",
      "\n",
      "\n",
      "[Recent Search]\n",
      "Aquaman :  🌊� kennis 🦈 (or 🐬 for dolphin if you prefer)\n"
     ]
    }
   ],
   "source": [
    "get_result('Superman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 🕷️👨‍⬛💥\n",
      "None\n",
      "\n",
      "\n",
      "[Recent Search]\n",
      "Superman :  👨\\u200d🚀🔵💥\\n\\n(Note: The blue S represents the \"S\" in Superman.)\n"
     ]
    }
   ],
   "source": [
    "get_result('Spiderman')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
